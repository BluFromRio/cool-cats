<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pet Image Classifier</title>
  <link rel="stylesheet" href="stylepage.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.4.2/p5.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/ml5@0.12.2/dist/ml5.min.js"></script>
</head>
<body>

  <!-- Header -->
  <header>
    <h1>Pet Identifier</h1>
    <nav>
      <div class="nav-home">
        <a href="index.html">Home</a>
      </div>
      <ul class="nav-links">
        <li><a href="#project-statement">Project Statement</a></li>
        <li><a href="#training">Training Process</a></li>
        <li><a href="#training-2">Video Demo</a></li>
        <li><a href="#demo">Classifier Demo</a></li>
        <li><a href="#personal-reflections">Our Personal Reflections</a></li>
        <li><a href="#references">Our References</a></li>
      </ul>
    </nav>
  </header>

  <!-- Banner -->
  <section class="banner">
    <h2>Classify Your Pet with Machine Learning</h2>
    <p>Drop an image of a pet into the classifier and let our model tell you the pet type!</p>
  </section>

  <!-- Main Content -->
  <main>
  
  
  <!-- Project Statement -->
  <section id="project-statement" class="content-card">
  <h2>Project Statement</h2>
  <div class="persontext" style="
      max-width: 700px; 
      margin: 0 auto; 
      max-height: 400px; 
      overflow-y: auto; 
      padding-right: 10px;">
    <p>The tech industry is dominated by the ‚ÄòCalifornia Ideology‚Äô, a belief ‚Äúthat the government [and] state bureaucracies are a challenge to individual freedom and economic growth‚Äù, it includes Ayn Rand-ism, civil libertarianism, and the notion of a ‚Äòtechno-elite‚Äô (Week 7 Lecture 5:10). This ideology has led to the pursuit of technological progress and profits without concern for how these technologies might harm others. In ‚ÄúTech, Heal Thyself‚Äù, Ellen Pao notes how tech companies were once pitched with noble goals such as Google‚Äôs goal to ‚ÄúOrganize the world's information and make it universally accessible and useful‚Äù, but over time venture capital started funding companies focused on replacing people with technology. When you believe that profit is an inherent moral good, it‚Äôs easy to toss away concerns about how your technology is or could harm people.</p>

    <p>This ‚ÄòCalifornia Ideology‚Äô was witnessed first-hand when a famous venture capital firm, Y Combinator, recently visited UW-Madison. They brought with them the founders of a company who is creating an AI Recruiter specifically for ‚Äúunder $30/hour‚Äù jobs. During the talk the founders of the company played a demo of their product. We listened to an awkward call where a lifeless AI voice repeatedly asked someone if they had 15 minutes to talk about the role. Later, during a question-and-answer section, someone asked them if it was ethical to have AI call people without them knowing. They responded with a non-answer about how their system was better than an ‚Äúoverworked recruiter‚Äù completely ignoring the question at hand. One of the Y Combinator employees chimed in and effectively said that if you aren‚Äôt one of the people building these technologies, you don‚Äôt get a say in if they are or aren‚Äôt ethical. This encounter highlights the fact that AI systems are being developed by people indifferent to ethical concerns.</p>

    <p>As tech companies bulldoze forward with AI technologies, it is important now more than ever to understand how these technologies harm people. Buolawmini started her book ‚ÄúUnmasking AI‚Äù with the anecdote of how off-the-shelf facial recognition technology couldn‚Äôt recognize her face but could if she was wearing a white mask (pg 9-10). This was an example of the ‚Äòcoded-gaze‚Äô a term she coined to describe ‚Äúthe ways in which the priorities, preferences, and prejudices of those who have the power to shape technology can propagate harm‚Äù (pg 10). Because the creators of these facial recognition tools didn‚Äôt have dark skin, they failed to consider people with dark skin in the testing and evaluation of their facial recognition too, leading to it performing noticeably worse on people with dark skin.</p>

    <p>Buolawmini couldn‚Äôt get off-the-shelf facial tracking technology to recognize her face, until she wore a white mask. The AI models used in these facial tracking software packages reflected the biases of the people that created them. Because the creators of these AI models were not testing them on black women and other minority groups, they were less effective at detecting their faces. When IBM became aware their model was misidentifying black women, they created an updated model that was better able to detect their faces increasing accuracy from 65.3% to 96.5%.</p>

    <p>However, in order to truly understand the coded-gaze, it must be analyzed through the lens of intersectionality. It is not enough to analyze how the coded-gaze affects people along one axis (race, gender, etc.), only by analyzing ‚Äúmany axes that work together and influence each other‚Äù can the coded-gaze be truly understood (Patricia Hill Collins & Sirma Bilge 2016 via Week 1 Lecture). An example of this is IBM‚Äôs Face++, while their facial recognition AI performed well on dark males (99.3% accuracy) and light skinned females (98.3% accuracy), it performed terribly on dark skinned females (65.3% accuracy) (Buolamwini 141). Simply analyzing along one axis would have missed this glaring equality: the AI performed well on people with dark skin and the AI performed well on women, but it didn‚Äôt perform well on dark skinned women. When IBM designed a new model with intersectionality in mind, they were able to improve the performance on darker females from 65.3% to 96.5% accuracy (Buolamwini 151).</p>

    <p>This lesson that defaults are not neutral from 'Unmasking AI' resonates with us, and as we create our own artificial intelligence systems, we want to try to be cognizant of the fact that biases in our training data will be reflected in our AI model. By training on and testing with many different kinds of pets, we reduce the risk of a pet getting misclassified by our model. While we will likely be unable to completely eliminate bias or error from our model, by ensuring we have diverse training and testing data, we reduce the amount of bias present in our model.</p>

    <p>Historical examples like the LDK camera, with settings for skin-tone variation, show that ‚Äúalternative systems can be made‚Äù (Buolamwini 60). AI systems have the potential to reinforce existing biases, as evidenced by Amazon‚Äôs failed hiring AI that when trained on existing hiring data, discriminated against women because ‚Äúthere are very, very few women working in powerful tech jobs at Amazon‚Äù (Kantayya 0:27:00-0:27:44). When designing our AI model we included many different kinds of pets, not just traditional pets such as cats and dogs. By including these pets, we increase the chance our model is able to recognize a less common pet, and therefore we are less likely to reinforce any ideas of what a ‚Äònormal‚Äô pet is. We are unlikely to be able to truly represent all pets, but by creating as large of a dataset as we can we reduce this bias.</p>

    <p>The inability to ever truly eliminate bias from our model emphasizes the importance of one of the boldest lessons from ‚ÄùUnmasking AI‚Äù: sometimes technologies are simply not worth creating or using. In a White House roundtable with then-president Joe Biden, Buolamwini urged Biden to ‚Äústop the use of facial recognition by the TSA for domestic flights specifically, but also its use for mass surveillance more broadly‚Äù (pg 286). AI tools can be extremely harmful for use cases such as policing and law enforcement. In situations where the harms of potential bias are too high or developers fail to remove bias from a tool, it is better to simply not create it than to make the world a worse place.</p>

    <p>With this in mind we wanted to create a tool with low potential to cause harm, ultimately choosing to make a tool for identifying pets. The correctness of a pet classification can be verified by the user of our tool after it makes a guess by researching more about the animal our tool thinks the pet is. If our tool makes an incorrect guess, after researching the animal, the user is ultimately just left where they started. The focus on classifying pets instead of people also lets us ensure that our tool can be equally used by all sorts of different people, since the model doesn‚Äôt directly depend on the person using the tool. We attempted to collect a large dataset representing many different pets, and feel our model is low enough risk to release as a demo on our website.</p>
  </div>
</section>

  
  

<!-- Model Training Process -->
<section id="training" class="content-card">
    <h2>Training the Model</h2>
    <div class="people">
      <div class="person">
        <!-- Left: Training description -->
        <div class="persontext">
          <p>
            When we started training the model, we began with Cheddar. He‚Äôs got a unique fluffy look, so it felt like a good first test case. After that we added Lulu, who has a
             similar energy, but shorter hair and slightly different posture.
          </p>
          <p>
            Then we added Jamey, who has similar coloring to Lulu, which helped the model get better at noticing more detailed features. To differentiate our classes, 
            we added Jasper, since he‚Äôs not a cat and has a very different appearance. That way, we could mix pet types and see how well the model handled variation.
          </p>
          <p>
            Our first iteration allowed users to identify only the provided pets, so for our final iteration we modified our classifications to classify not just specific pets
             but instead generally identify their pets. At first it was really hard to find free images for a lot of different pets such as guinea pigs, so we reduced our classes to specifically:
             Cats, Dogs, Bunnies, Fish, Snakes, and Birds.
          </p>
          <p>
            We quickly found a lot of limitations with our classifier. For instance, small white dogs and cats are often mistaken for bunnies. As well,
             fluffy dogs can be mistake as cats. We tried to combat the fluffy dog issue by adding samples of Corey who has some fluff to him. This helped
             in a lot of cases but there are still some gaps. Finally, we couldn‚Äôt find many samples of fish and snakes so the classifer runs into a lot of issues 
             when it comes to classifying fish and snakes. Still the classifer has gotten some very clear detailed images correct.
          </p>
        </div>
  
        <!-- Right: Image -->
        <div class="training-image-stack">
          <img src="process.jpg" alt="Screenshot..." class="training-preview"
            style="margin: 4rem auto 0 auto;" />
          <img src="Mistake.jpg" alt="Screenshot of mistake in training the model in Teachable Machine"
            class="training-preview"
            style="height: 300px; width: auto; margin: 10rem auto 0 auto;" />
        </div>
      </div>
    </div>
  </section>

<!-- Model Training Process Video Portion -->
<section id="training-2" class="content-card">
    <h2>Video Demo</h2>
    <div class="people">
      <div class="person">
        <!-- Left: Training description -->
        <div class="persontext">
          <p>
            In this demo we go through all the pets types: Cat, Dog, Bunny, Fish, Bird, and Snake.
          </p>
          <p>
            The video is just a demonstration of how the machine works under the hood
          </p>
        </div>
  
        <!-- Right: Image + Video stacked -->
        <div class="personimage">
            <video controls style="width: 120%; max-width: 500px; border-radius: 10px;">
                <source src="FinalDemo.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
        </div>
      </div>
    </div>
  </section>
  
<!-- Combined Demo + Explanation -->
<section id="demo" class="content-card">
  <h2>Try It Out!</h2>
  <div class="people">
    <div class="person">
      <!-- Left side: explanation with sub-header -->
      <div class="persontext">
        <h3>How It Works</h3>
        <p>
          This tool uses a machine learning model trained in Google's Teachable Machine. We've used ml5.js and p5.js to integrate
          that model into a web page, allowing you to classify images of pets by dragging them into a central drop zone.
        </p>
        <p>
          The model has been trained on specific photos of our pets‚Äî Jasper, Cheddar, Jamey, Corey, Charlie, Bunners, Kilamanjaro,
           Paisley, Selena, Toaster, Lemon, Coy, Lulu, and a few more friends. Try dropping one in to see
          if the classifier gets it right!
        </p>
        <p>
          If you want to test our classifier yourself, you can upload your own image with the "Choose File" option, or you can switch to camera mode and show your pet to the camera!
          Just make sure to switch back to upload mode if you want to go back to uploading your own images or dropping in one of our pets.
        </p>
      </div>

      <!-- Right side: classifier UI -->
      <div class="personimage">
        <div id="input-switch" style="text-align: center; margin-bottom: 1rem;">
          <button onclick="setInputMode('image')"> Upload Mode</button>
          <button onclick="setInputMode('camera')"> Camera Mode</button>
          <input type="file" id="fileUpload" accept="image/*" style="margin-bottom: 1rem;" />
        </div>          
        <div id="canvas-container"></div>
        <p id="result">Prediction: ...</p>

        <div class="tiles">
          <div class="tile-wrapper">
            <div class="tile jasper" draggable="true" data-src="jasper.jpg"></div>
            <div class="tile-label">Jasper</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile cheddar" draggable="true" data-src="cheddar.jpg"></div>
            <div class="tile-label">Cheddar</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile jamey" draggable="true" data-src="jamey.jpg"></div>
            <div class="tile-label">Jamey</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile lulu" draggable="true" data-src="lulu.jpg"></div>
            <div class="tile-label">Lulu</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Corey" draggable="true" data-src="Corey.jpg"></div>
            <div class="tile-label">Corey</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Charlie" draggable="true" data-src="Charlie.jpg"></div>
            <div class="tile-label">Charlie</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Bunners" draggable="true" data-src="Bunners.jpg"></div>
            <div class="tile-label">Bunners</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Coy" draggable="true" data-src="Coy.jpg"></div>
            <div class="tile-label">Coy</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Lemon" draggable="true" data-src="Lemon.jpg"></div>
            <div class="tile-label">Lemon</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Toaster" draggable="true" data-src="Toaster.jpg"></div>
            <div class="tile-label">Toaster</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Selena" draggable="true" data-src="Selena.jpg"></div>
            <div class="tile-label">Selena</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Paisley" draggable="true" data-src="Paisley.jpg"></div>
            <div class="tile-label">Paisley</div>
          </div>
          <div class="tile-wrapper">
            <div class="tile Kilamanjaro" draggable="true" data-src="Kilamanjaro.jpg"></div>
            <div class="tile-label">Kilamanjaro</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Personal Reflections -->
<section id="personal-reflections" class="content-card">
  <h2>Personal Reflections</h2>
  <div class="persontext" style="max-width: 1000px; margin: 0 auto; display: flex; flex-wrap: wrap; gap: 40px; justify-content: center;">

    <div style="flex: 1 1 400px; min-width: 300px;">
      <h3>Human Made Art is Vital - Tristan</h3>
      <p>This project made me think a lot about where AI fits into everything. Not just tech, but art, identity, even the stuff we believe about progress. The idea that tech is neutral felt kind of ridiculous by the end of all this. AI just copies what already exists, including all the bias, and puts it into something that looks shiny and new. Like facial recognition failing Black and Brown women or hiring tools that reflect old patterns instead of breaking them. It‚Äôs not some mistake, it‚Äôs the system working exactly how it was trained to.</p>

      <p>On the creative side, I‚Äôve always loved art because it feels personal. Whether it‚Äôs manga, street art, or something weird in a museum, there‚Äôs a human behind it. AI can‚Äôt do that. It can remix and copy and even trick people sometimes, but it can‚Äôt feel. It doesn‚Äôt understand context or pain or joy. It just predicts what should come next. That‚Äôs not creativity. That‚Äôs code. I don‚Äôt want a world where the things that move us are made without feeling.</p>
      <p>AI art "is an insult to life itself" - Hayao Miyazaki the co-founder, director, and creative visionary behind Studio Ghibli.</p>
    </div>

    <div style="flex: 1 1 400px; min-width: 300px;">
      <h3>Biased Data, Same Problems - Ben</h3>
      <p>What stood out to me most was how artificial intelligence depends entirely on the past. Every system we looked at was just repeating old data in new forms. And when that data is biased, the system becomes biased. It‚Äôs not just about who builds the tech, it‚Äôs also about the information we feed into it. Amazon‚Äôs AI didn‚Äôt hate women. It was just trained on hiring data that already did. That‚Äôs the problem.</p>

      <p>I also kept coming back to the question of moral imagination. Can a machine ever have it? Right now, AI reacts to patterns, but it doesn‚Äôt understand them. It can use language or visuals that trigger an emotion, but it‚Äôs not the same as creating something from experience. That difference matters. Especially when AI is being used in things like policing, hiring, or even making art. It‚Äôs easy to get caught up in the hype. But we need to slow down and ask who this tech is serving, and at what cost.</p>
    </div>

  </div>
</section>

  <!-- References -->
  <section id="references" class="content-card">
    <h2>References</h2>
    <ul style="list-style-type: none; padding: 0; font-size: 18px;">
      <li>üîó <a href="https://teachablemachine.withgoogle.com/" target="_blank">Teachable Machine</a></li>
      <li>üíæ <a href="https://www.tensorflow.org/js" target="_blank">TensorFlow.js</a></li>
      <li>üíª <a href="https://ml5js.org/" target="_blank">ml5.js</a></li>
      <li>üíª <a href="https://image-net.org/" target="_blank">Image-Net</a></li>
      <li>üìì <a href="https://arxiv.org/abs/1704.04861" target="_blank">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></li>
    </ul>
  </section>

  </main>

  <!-- JS Classifier Logic -->
  <script>
    let classifier;
    let label = "Waiting for image...";
    let droppedImage;
    let imgReady = false;
    let modelURL = "https://teachablemachine.withgoogle.com/models/sIFIYOrPC/";
    let modelURL2 = "https://teachablemachine.withgoogle.com/models/Als7ZN0pl/";
    let inputMode = "image"; // "image" or "camera"
    let video;


    function preload() {
      classifier = ml5.imageClassifier(modelURL + "model.json");
    }

    function setup() {
      let canvas = createCanvas(300, 300);
      canvas.parent("canvas-container");
      background(30);
      textAlign(CENTER, CENTER);
      textSize(16);
      fill(255);
      text("Drop a pet image here", width / 2, height / 2);

      const tiles = document.querySelectorAll('.tile');
      const imageMap = {
        "jasper.jpg": "jasper.jpg",
        "cheddar.jpg": "cheddar.jpg",
        "jamey.jpg": "jamey.jpg",
        "lulu.jpg": "lulu.jpg",
        "Kilamanjaro.jpg": "Kilamanjaro.jpg",
        "Paisley.jpg": "Paisley.jpg",
        "Corey.jpg": "Corey.jpg",
        "Selena.jpg": "Selena.jpg",
        "Toaster.jpg": "Toaster.jpg",
        "Lemon.jpg": "Lemon.jpg",
        "Charlie.jpg": "Charlie.jpg",
        "Bunners.jpg": "Bunners.jpg",
        "Coy.jpg": "Coy.jpg"
      };

      tiles.forEach(tile => {
        const src = tile.getAttribute("data-src");
        tile.style.backgroundImage = `url('${imageMap[src]}')`;

        tile.addEventListener('dragstart', e => {
          e.dataTransfer.setData("src", src);
        });
      });

      const dropTarget = document.getElementById("canvas-container");
      dropTarget.addEventListener('dragover', e => e.preventDefault());

      dropTarget.addEventListener('drop', e => {
        e.preventDefault();
        const src = e.dataTransfer.getData("src");

        loadImage(src, img => {
          droppedImage = img;
          imgReady = true;
        });
      });
    }

    function setInputMode(mode) {
        console.log("Mode changed to:", mode);
        alert("Switched to " + mode + " mode");
      inputMode = mode;
      document.getElementById("result").innerText = "Prediction: ...";

      if (mode === "camera") {
        // Remove old video if it exists
        if (video) {
          video.remove();
          video = null;
        }

        noLoop(); // if you're drawing frames manually
        video = createCapture(VIDEO, () => {
        video.size(300, 300);
        video.hide();
        video.parent("canvas-container");
        loop(); // re-enable loop once video is ready
        });

      } else {
        // remove video if going back to upload
        if (video) {
          video.remove();
          video = null;
        }

        clear();
        background(30);
        fill(255);
        textAlign(CENTER, CENTER);
        text("Drop a pet image here", width / 2, height / 2);
      }
    }

    function draw() {
      if (inputMode === "image") {
        if (imgReady && droppedImage) {
          background(0);
          image(droppedImage, 0, 0, width, height);
          classifier.classify(droppedImage, gotResult);
          imgReady = false;
        }
      } else if (inputMode === "camera" && video) {
        image(video, 0, 0, width, height);
        classifier.classify(video, gotResult);
      }
    }

    document.getElementById("fileUpload").addEventListener("change", function(event) {
      if (inputMode !== "image") return;
      const file = event.target.files[0];
      if (file) {
        const imgURL = URL.createObjectURL(file);
        loadImage(imgURL, img => {
          droppedImage = img;
          imgReady = true;
        });
      }
    });

    function gotResult(error, results) {
      if (error) {
        console.error(error);
        label = "Error!";
        return;
      }

      label = "Prediction: " + results[0].label;
      document.getElementById("result").innerText = label;
    }
  </script>

</body>
</html>
